# ğŸ“ Complete Beginner's Guide to LangGraph on Databricks

**A step-by-step tutorial for building your first AI agent - no prior experience required!**

---

## ğŸ“– Table of Contents

1. [What You'll Build](#what-youll-build)
2. [Prerequisites](#prerequisites)
3. [Understanding the Basics](#understanding-the-basics)
4. [Step 1: Set Up Your Databricks Environment](#step-1-set-up-your-databricks-environment)
5. [Step 2: Deploy Using Databricks Asset Bundles](#step-2-deploy-using-databricks-asset-bundles)
6. [Step 3: Configure Your Unity Catalog](#step-3-configure-your-unity-catalog)
7. [Step 4: Upload and Run the Tutorial Notebook](#step-4-upload-and-run-the-tutorial-notebook)
8. [Step 5: Test Individual Tools](#step-5-test-individual-tools)
9. [Step 6: Build Your First LangChain Tool](#step-6-build-your-first-langchain-tool)
10. [Step 7: Create a Simple Agent](#step-7-create-a-simple-agent)
11. [Step 8: Build a LangGraph ReAct Agent](#step-8-build-a-langgraph-react-agent)
12. [Step 9: Compare and Test](#step-9-compare-and-test)
13. [Troubleshooting](#troubleshooting)
14. [Next Steps](#next-steps)

---

## ğŸ¯ What You'll Build

By the end of this tutorial, you will have built an **intelligent AI agent** that can:

- ğŸ“ Classify support tickets automatically
- ğŸ“Š Extract key information from text
- ğŸ” Search a knowledge base for solutions
- ğŸ“ˆ Query historical data using natural language
- ğŸ¤– Decide which tools to use based on the task

**Time Required**: 2-3 hours for complete beginners

---

## âœ… Prerequisites

### Required Accounts & Access

- [ ] **Databricks Account** (Free trial available at [databricks.com/try-databricks](https://www.databricks.com/try-databricks))
  - Works on AWS, Azure, or GCP
- [ ] **Unity Catalog Enabled** (Usually enabled by default in new workspaces)
- [ ] **Basic Computer Skills** (Can navigate folders, copy/paste text)

### What You DON'T Need

- âŒ Prior AI/ML experience
- âŒ Python expertise (we'll explain everything)
- âŒ LangChain knowledge (we'll teach you)
- âŒ Complex setup (automated with Databricks Asset Bundles)

---

## ğŸ“š Understanding the Basics

Before we start coding, let's understand what we're building.

### What is an AI Agent?

An **AI agent** is a program that:
1. **Thinks** - Understands what you want
2. **Acts** - Uses tools to get information
3. **Decides** - Chooses the best approach
4. **Responds** - Gives you an answer

**Real-world analogy**: Like a smart assistant who knows when to check your calendar, search Google, or ask a colleague.

### What is LangChain?

**LangChain** is a framework that makes it easy to build applications with Large Language Models (LLMs).

Think of it as a **toolbox** that includes:
- Ways to talk to AI models (like ChatGPT)
- Tools that AI can use (search, calculate, etc.)
- Memory to remember conversations

### What is LangGraph?

**LangGraph** extends LangChain to build agents that can:
- Make multiple decisions in a loop
- Use different tools based on what they learn
- Handle complex, multi-step tasks

**Key concept**: The **ReAct pattern** (Reasoning + Acting)
```
User asks: "What's urgent?"
â†“
Agent THINKS: "I need to classify tickets first"
â†“
Agent ACTS: Calls classify_ticket()
â†“
Agent OBSERVES: Result shows P1 critical issue
â†“
Agent THINKS: "Now I should search for solutions"
â†“
Agent ACTS: Calls search_knowledge()
â†“
Agent RESPONDS: "Found solution KB-123"
```

### What are Databricks Asset Bundles?

**Databricks Asset Bundles (DAB)** are a way to:
- Package your notebooks, configs, and dependencies
- Deploy everything with ONE command
- Version control your entire project
- Switch between dev/staging/prod environments

**Analogy**: Like a "recipe box" that contains everything needed to cook a meal (ingredients, instructions, tools).

---

## ğŸš€ Step 1: Set Up Your Databricks Environment

### 1.1 Log into Databricks

1. Go to your Databricks workspace URL
   - Example: `https://adb-1234567890123456.7.azuredatabricks.net/`
2. Log in with your credentials

### 1.2 Create a Cluster (If You Don't Have One)

A **cluster** is a group of computers that run your code.

1. Click **Compute** in the left sidebar
2. Click **Create Cluster**
3. Configure:
   - **Cluster name**: `langraph-tutorial`
   - **Cluster mode**: `Single Node` (cheapest for learning)
   - **Databricks Runtime**: `16.4 LTS ML` or later
   - **Node type**: `Standard_DS3_v2` (or smallest available)
4. Click **Create Cluster**
5. Wait 3-5 minutes for it to start (green circle = ready)

**ğŸ’¡ Tip**: Write down your **Cluster ID** - you'll need it later!
- Find it in the cluster URL: `https://.../clusters/1234-567890-abcde123`
- Cluster ID = `1234-567890-abcde123`

### 1.3 Create a Catalog and Schema

A **catalog** is like a database, and a **schema** is like a folder inside it.

1. Click **Catalog** in the left sidebar
2. Click **Create Catalog**
3. Name it: `langraph_tutorial` (lowercase, no spaces)
4. Click **Create**
5. Click on your new catalog
6. Click **Create Schema**
7. Name it: `agents` (lowercase)
8. Click **Create**

**âœ… You now have**: `langraph_tutorial.agents` (write this down!)

### 1.4 Get Your SQL Warehouse ID

A **SQL Warehouse** runs SQL queries (needed for some features).

1. Click **SQL Warehouses** in the left sidebar
2. If you don't have one, click **Create SQL Warehouse**
3. Click on your warehouse name
4. Copy the **Warehouse ID** from the URL
   - Example: `/sql/warehouses/abc123def456`
   - Warehouse ID = `abc123def456`

**âœ… Write down**:
- âœï¸ Cluster ID: `____________________`
- âœï¸ Catalog: `langraph_tutorial`
- âœï¸ Schema: `agents`
- âœï¸ Warehouse ID: `____________________`

---

## ğŸ“¦ Step 2: Deploy Using Databricks Asset Bundles

### 2.1 Clone the Tutorial Repository

**Option A: Using Git in Databricks (Recommended)**

1. Click **Repos** in the left sidebar
2. Click **Add Repo**
3. Enter:
   - **Git repo URL**: `https://github.com/bigdatavik/databricks-ai-ticket-vectorsearch.git`
   - **Git provider**: `GitHub`
   - **Repo name**: `langraph-tutorial`
4. Click **Create**
5. After it clones, click the **branch dropdown** (says "main")
6. Search for `langgraph-databricks-tutorial`
7. Click to switch to that branch

**Option B: Using Command Line**

If you have Databricks CLI installed:

```bash
# Clone the repository
git clone https://github.com/bigdatavik/databricks-ai-ticket-vectorsearch.git
cd databricks-ai-ticket-vectorsearch

# Switch to tutorial branch
git checkout langgraph-databricks-tutorial
```

### 2.2 Install Databricks CLI

The **Databricks CLI** lets you deploy from your computer.

**On Mac/Linux:**
```bash
curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
```

**On Windows:**
```powershell
# Using PowerShell
winget install Databricks.DatabricksCLI
```

**Verify installation:**
```bash
databricks --version
# Should show: Databricks CLI v0.x.x
```

### 2.3 Configure Databricks CLI

Connect the CLI to your workspace:

```bash
databricks configure --token
```

You'll be prompted for:
1. **Databricks Host**: Your workspace URL
   - Example: `https://adb-1234567890123456.7.azuredatabricks.net/`
2. **Token**: Create one by:
   - Click your profile icon (top right)
   - Click **User Settings**
   - Click **Developer** â†’ **Access Tokens**
   - Click **Generate New Token**
   - Name it: `langraph-tutorial`
   - Click **Generate**
   - **Copy the token** (you can't see it again!)
   - Paste it in the CLI prompt

**âœ… CLI is now connected to your workspace!**

### 2.4 Update the Bundle Configuration

Now we'll customize the bundle for YOUR workspace.

1. Open `databricks.yml` in the repository
2. Update these values:

```yaml
bundle:
  name: langraph-tutorial

variables:
  catalog:
    default: langraph_tutorial          # â† YOUR CATALOG NAME
  schema:
    default: agents                      # â† YOUR SCHEMA NAME
  cluster_id:
    default: "1234-567890-abcde123"     # â† YOUR CLUSTER ID
  warehouse_id:
    default: "abc123def456"             # â† YOUR WAREHOUSE ID

workspace:
  host: https://your-workspace.cloud.databricks.com  # â† YOUR WORKSPACE URL
```

3. Save the file

### 2.5 Validate the Bundle

Make sure everything is configured correctly:

```bash
databricks bundle validate
```

**Expected output:**
```
âœ“ Configuration is valid
```

**If you see errors**, check:
- Cluster ID is correct (no quotes in YAML)
- Catalog and schema exist
- Workspace URL is correct

### 2.6 Deploy the Bundle

**ğŸ‰ Deploy everything with ONE command:**

```bash
databricks bundle deploy
```

**What this does:**
- Uploads the tutorial notebook to your workspace
- Creates necessary configurations
- Sets up permissions
- Installs dependencies

**Expected output:**
```
âœ“ Uploaded tutorial/23_langraph_agent_learning.py
âœ“ Uploaded docs/
âœ“ Deployment complete!

Your notebook is available at:
/Workspace/Users/your-email@example.com/.bundle/langraph-tutorial/dev/tutorial/23_langraph_agent_learning
```

**âœ… Copy this notebook path - you'll need it!**

---

## ğŸ”§ Step 3: Configure Your Unity Catalog

Before running the notebook, we need to set up some AI functions.

### 3.1 What Are Unity Catalog AI Functions?

**Unity Catalog AI Functions** are pre-built AI capabilities you can use like regular SQL functions:
- `ai_classify()` - Categorizes text
- `ai_extract()` - Pulls out key information
- `ai_gen()` - Generates text

**Analogy**: Like Excel functions (SUM, AVERAGE), but for AI tasks!

### 3.2 Create AI Functions

We'll create these using SQL (don't worry, I'll give you the exact commands).

1. Click **SQL Editor** in the left sidebar (or use a notebook)
2. Copy and paste these commands one at a time:

**A. Classification Function**
```sql
-- Switch to your catalog
USE CATALOG langraph_tutorial;
USE SCHEMA agents;

-- Create classification function
CREATE OR REPLACE FUNCTION ai_classify(
  ticket_text STRING COMMENT 'The support ticket text to classify'
)
RETURNS STRUCT<
  priority: STRING COMMENT 'Priority: P1, P2, P3, P4',
  category: STRING COMMENT 'Category: Access, Database, Network, Application, etc.',
  confidence: STRING COMMENT 'Confidence: High, Medium, Low'
>
COMMENT 'Classifies support tickets by priority and category'
RETURN ai_query(
  'databricks-meta-llama-3-1-70b-instruct',
  CONCAT(
    'Classify this support ticket into priority (P1-P4) and category. ',
    'P1=Critical/Outage, P2=High/Degraded, P3=Medium/Issue, P4=Low/Question. ',
    'Categories: Access, Database, Network, Application, Infrastructure, Security, Other. ',
    'Ticket: ', ticket_text
  )
);
```

**B. Extraction Function**
```sql
-- Create extraction function
CREATE OR REPLACE FUNCTION ai_extract(
  ticket_text STRING COMMENT 'The support ticket text to analyze'
)
RETURNS STRUCT<
  user_email: STRING COMMENT 'User email if mentioned',
  system_name: STRING COMMENT 'System or application name',
  error_code: STRING COMMENT 'Error code if mentioned',
  urgency_indicators: STRING COMMENT 'Words indicating urgency'
>
COMMENT 'Extracts key metadata from support tickets'
RETURN ai_query(
  'databricks-meta-llama-3-1-70b-instruct',
  CONCAT(
    'Extract structured information from this support ticket: ',
    'user email, system name, error code, urgency indicators. ',
    'Ticket: ', ticket_text
  )
);
```

**C. Response Generation Function**
```sql
-- Create generation function
CREATE OR REPLACE FUNCTION ai_gen(
  ticket_text STRING COMMENT 'The support ticket text',
  context STRING COMMENT 'Knowledge base context or solutions'
)
RETURNS STRING
COMMENT 'Generates a helpful response based on ticket and context'
RETURN ai_query(
  'databricks-meta-llama-3-1-70b-instruct',
  CONCAT(
    'Generate a helpful support response based on this ticket and context. ',
    'Ticket: ', ticket_text, ' ',
    'Context: ', context
  )
);
```

**âœ… Run each command and verify**:
```sql
-- Test classification
SELECT ai_classify('URGENT: Database server down, users cannot access production DB');

-- You should see results like:
-- priority: "P1"
-- category: "Database"
-- confidence: "High"
```

**ğŸ’¡ Troubleshooting**:
- If you get "Function not found", make sure you ran `USE CATALOG` and `USE SCHEMA` first
- If you get permission errors, ask your workspace admin for `CREATE FUNCTION` permission

---

## ğŸ““ Step 4: Upload and Run the Tutorial Notebook

### 4.1 Open the Notebook

If you deployed with bundles:
1. Click **Workspace** in the left sidebar
2. Navigate to the path from Step 2.6
3. Click on `23_langraph_agent_learning.py`

If you cloned via Repos:
1. Click **Repos** in the left sidebar
2. Click your `langraph-tutorial` repo
3. Navigate to `tutorial/23_langraph_agent_learning.py`

### 4.2 Attach to Your Cluster

1. At the top of the notebook, click the **cluster dropdown**
2. Select your `langraph-tutorial` cluster
3. Wait for it to connect (green checkmark)

### 4.3 Understand the Notebook Structure

The notebook is divided into sections:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Section 1: Setup & Installation       â•‘
â•‘  â”œâ”€ Install LangGraph                  â•‘
â•‘  â”œâ”€ Install LangChain                  â•‘
â•‘  â””â”€ Import libraries                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Section 2: Configure Environment      â•‘
â•‘  â”œâ”€ Set CATALOG, SCHEMA                â•‘
â•‘  â”œâ”€ Set CLUSTER_ID, WAREHOUSE_ID       â•‘
â•‘  â””â”€ Initialize WorkspaceClient         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Section 3: Test Individual Tools      â•‘
â•‘  â”œâ”€ Test ai_classify()                 â•‘
â•‘  â”œâ”€ Test ai_extract()                  â•‘
â•‘  â”œâ”€ Test Vector Search                 â•‘
â•‘  â””â”€ Test Genie API                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Section 4: Create LangChain Tools     â•‘
â•‘  â”œâ”€ Define Pydantic input schemas      â•‘
â•‘  â”œâ”€ Wrap each function as a Tool       â•‘
â•‘  â””â”€ Test tools individually            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Section 5: Build Sequential Agent     â•‘
â•‘  â”œâ”€ Create fixed 4-step workflow       â•‘
â•‘  â”œâ”€ Run all tools in sequence          â•‘
â•‘  â””â”€ Test with sample ticket            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Section 6: Build LangGraph ReAct      â•‘
â•‘  â”œâ”€ Initialize LLM with bind_tools()   â•‘
â•‘  â”œâ”€ Create ReAct agent                 â•‘
â•‘  â”œâ”€ Test adaptive tool selection       â•‘
â•‘  â””â”€ Compare with sequential            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Section 7: Advanced Patterns          â•‘
â•‘  â”œâ”€ Error handling                     â•‘
â•‘  â”œâ”€ Retry logic                        â•‘
â•‘  â””â”€ Best practices                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 4.4 Configure the Notebook

Find **Cmd 2** (usually near the top) and update:

```python
# ========================================
# CONFIGURATION - Update these values!
# ========================================

CATALOG = "langraph_tutorial"              # â† YOUR CATALOG
SCHEMA = "agents"                          # â† YOUR SCHEMA
WAREHOUSE_ID = "abc123def456"              # â† YOUR WAREHOUSE ID
CLUSTER_ID = "1234-567890-abcde123"        # â† YOUR CLUSTER ID

# Optional: Vector Search (we'll set up later)
VECTOR_SEARCH_ENDPOINT = "your-endpoint"   # â† Leave as-is for now
VECTOR_SEARCH_INDEX = "your-index"         # â† Leave as-is for now

# Optional: Genie (we'll set up later)
GENIE_SPACE_ID = "your-space-id"           # â† Leave as-is for now
```

---

## ğŸ§ª Step 5: Test Individual Tools

Now let's verify each tool works before building the agent.

### 5.1 Test AI Classification

Run the cell that tests `ai_classify()`:

```python
# Test classification function
test_ticket = "URGENT: Production database is down! All users are affected."

result = spark.sql(f"""
    SELECT ai_classify('{test_ticket}') as classification
""").collect()[0]

print(f"Priority: {result.classification.priority}")
print(f"Category: {result.classification.category}")
print(f"Confidence: {result.classification.confidence}")
```

**Expected output:**
```
Priority: P1
Category: Database
Confidence: High
```

**âœ… What this means**:
- The AI correctly identified this as a **critical (P1)** issue
- It's a **Database** problem
- The AI is **highly confident** in this classification

### 5.2 Test AI Extraction

Run the cell that tests `ai_extract()`:

```python
# Test extraction function
test_ticket = "User john.doe@company.com reports error ERR-503 in PayrollApp system"

result = spark.sql(f"""
    SELECT ai_extract('{test_ticket}') as metadata
""").collect()[0]

print(f"User Email: {result.metadata.user_email}")
print(f"System: {result.metadata.system_name}")
print(f"Error Code: {result.metadata.error_code}")
```

**Expected output:**
```
User Email: john.doe@company.com
System: PayrollApp
Error Code: ERR-503
```

**âœ… What this means**:
- The AI extracted structured data from unstructured text
- It identified the user, system, and error code
- This would normally require complex regex patterns!

### 5.3 Understanding What You Just Did

**ğŸ“ Learning Point**:

You just used **AI Functions** that:
1. Take natural language as input
2. Use a Large Language Model (LLM) to understand it
3. Return structured data you can use in code

**Traditional approach** (without AI):
```python
# Would need hundreds of lines of code:
if "urgent" in text or "critical" in text:
    priority = "P1"
elif "important" in text:
    priority = "P2"
# ... and so on for every possible variation
```

**AI approach**:
```python
# One line:
ai_classify(text)  # Understands context automatically!
```

---

## ğŸ”¨ Step 6: Build Your First LangChain Tool

Now let's wrap these functions so LangChain can use them.

### 6.1 Understand Tool Structure

A **LangChain Tool** has three parts:

1. **Name** - What the tool is called
2. **Description** - What it does (the AI reads this!)
3. **Function** - The actual code to run

**Example structure:**
```python
@tool
def my_tool(input: str) -> str:
    """
    This description tells the AI what this tool does.
    The AI will read this to decide when to use it!
    
    Args:
        input: Description of the input parameter
    
    Returns:
        Description of what this returns
    """
    # Your code here
    return result
```

### 6.2 Create the Classification Tool

Find the section "Create LangChain Tools" and run this cell:

```python
from langchain.tools import tool
from pydantic import BaseModel, Field

# Define the input schema
class ClassifyInput(BaseModel):
    """Input for ticket classification tool"""
    ticket_text: str = Field(
        description="The support ticket text to classify"
    )

# Create the tool
@tool(args_schema=ClassifyInput)
def classify_ticket(ticket_text: str) -> dict:
    """
    Classifies a support ticket by priority (P1-P4) and category.
    
    Use this tool when you need to determine:
    - How urgent a ticket is (P1=Critical, P2=High, P3=Medium, P4=Low)
    - What category it belongs to (Database, Network, Access, etc.)
    
    Args:
        ticket_text: The full text of the support ticket
    
    Returns:
        Dictionary with priority, category, and confidence level
    """
    result = spark.sql(f"""
        SELECT ai_classify('{ticket_text}') as classification
    """).collect()[0]
    
    return {
        "priority": result.classification.priority,
        "category": result.classification.category,
        "confidence": result.classification.confidence
    }

# Test the tool
test_result = classify_ticket.invoke({
    "ticket_text": "Database connection timeout in production"
})
print("Tool Result:", test_result)
```

**Expected output:**
```python
Tool Result: {
    'priority': 'P2',
    'category': 'Database',
    'confidence': 'High'
}
```

### 6.3 Understanding What You Built

**ğŸ“ Key Concepts**:

1. **Pydantic Schema** (`ClassifyInput`):
   - Defines what inputs the tool expects
   - Provides type checking (ensures ticket_text is a string)
   - Gives descriptions the AI can read

2. **@tool Decorator**:
   - Tells LangChain "this is a tool"
   - Makes it available to AI agents
   - Handles serialization automatically

3. **Docstring** (the `""" """` part):
   - **CRITICAL**: The AI reads this to decide when to use the tool!
   - Must clearly explain: what it does, when to use it, what it returns
   - Like writing instructions for a human assistant

**Real-world analogy**:
```
You're hiring an assistant and giving them tools:

Tool: Stapler
Description: "Use this to attach papers together. 
             Use when you need to keep documents in order."
Function: *staples papers*

The AI reads descriptions like this to choose the right tool!
```

### 6.4 Create the Remaining Tools

Run the cells for the other three tools:
- `extract_metadata()` - Pulls out key information
- `search_knowledge()` - Searches for solutions
- `query_historical()` - Finds similar past tickets

**ğŸ’¡ Exercise**: Read each tool's docstring. Can you understand when the AI should use each one?

---

## ğŸ¤– Step 7: Create a Simple Agent

Let's start with a **sequential agent** (runs all tools in order).

### 7.1 What is a Sequential Agent?

A **sequential agent**:
- Runs tools in a fixed order (1, 2, 3, 4)
- Always runs ALL tools
- Predictable but not efficient

**Flow diagram:**
```
User Question
    â†“
Tool 1: Classify â†’ "P2, Database"
    â†“
Tool 2: Extract â†’ "user@email.com, ERR-503"
    â†“
Tool 3: Search KB â†’ "Solution KB-123"
    â†“
Tool 4: Query History â†’ "5 similar tickets"
    â†“
Agent Response
```

### 7.2 Build the Sequential Agent

Run this cell:

```python
from langchain_community.chat_models import ChatDatabricks

# Initialize the LLM
llm = ChatDatabricks(
    endpoint="databricks-meta-llama-3-1-70b-instruct",
    max_tokens=500
)

# Define the sequential workflow
def sequential_agent(question: str) -> str:
    """
    Run all tools in sequence and combine results
    """
    results = {}
    
    # Step 1: Classify
    print("ğŸ” Step 1: Classifying ticket...")
    results['classification'] = classify_ticket.invoke({"ticket_text": question})
    
    # Step 2: Extract
    print("ğŸ” Step 2: Extracting metadata...")
    results['metadata'] = extract_metadata.invoke({"ticket_text": question})
    
    # Step 3: Search KB
    print("ğŸ” Step 3: Searching knowledge base...")
    results['kb_results'] = search_knowledge.invoke({
        "query": question,
        "top_k": 3
    })
    
    # Step 4: Query History
    print("ğŸ” Step 4: Querying historical tickets...")
    results['history'] = query_historical.invoke({
        "category": results['classification']['category'],
        "priority": results['classification']['priority']
    })
    
    # Combine and generate response
    print("ğŸ’¡ Generating response...")
    context = f"""
    Classification: {results['classification']}
    Metadata: {results['metadata']}
    Knowledge Base: {results['kb_results']}
    Similar Tickets: {results['history']}
    """
    
    response = llm.invoke(f"""
    Based on this support ticket analysis, provide a helpful response:
    
    {context}
    
    Original Question: {question}
    """)
    
    return response.content

# Test it!
test_ticket = "Password reset needed for john.doe@company.com"
response = sequential_agent(test_ticket)
print("\nâœ… Agent Response:")
print(response)
```

### 7.3 Understanding the Output

You should see:
```
ğŸ” Step 1: Classifying ticket...
ğŸ” Step 2: Extracting metadata...
ğŸ” Step 3: Searching knowledge base...
ğŸ” Step 4: Querying historical tickets...
ğŸ’¡ Generating response...

âœ… Agent Response:
Based on the analysis, this is a P4 (Low priority) Access request.
For password resets, follow KB-001: Self-Service Password Reset Guide.
The user can reset their password at: https://portal.company.com/reset
Estimated resolution time: 5 minutes.
```

**ğŸ“ What just happened**:
1. The agent ran all 4 tools automatically
2. It gathered information from each tool
3. It used an LLM to combine the results into a helpful response
4. All of this happened without human intervention!

### 7.4 The Problem with Sequential Agents

Try this simple question:
```python
response = sequential_agent("What is your name?")
```

**Problem**: The agent will still run ALL 4 tools, even though none are needed!

- âŒ Wastes time (runs unnecessary tools)
- âŒ Wastes money (each tool call costs $)
- âŒ Not intelligent (can't adapt)

**This is where LangGraph comes in!**

---

## ğŸ§  Step 8: Build a LangGraph ReAct Agent

Now let's build an **adaptive agent** that chooses which tools to use.

### 8.1 What is ReAct?

**ReAct** = **Rea**soning + **Act**ing

The agent follows this loop:
```
1. THINK: What do I need to know?
2. ACT: Choose and use a tool
3. OBSERVE: What did the tool tell me?
4. DECIDE: Do I need more info, or can I answer?
5. REPEAT or RESPOND
```

**Example:**
```
User: "Password reset for john@company.com"

Agent THINKS: "This is about password reset, I should classify it first"
Agent ACTS: classify_ticket() â†’ "P4, Access"
Agent OBSERVES: "Low priority access request"
Agent THINKS: "For access requests, I should search the knowledge base"
Agent ACTS: search_knowledge() â†’ "KB-001: Password Reset Guide"
Agent OBSERVES: "Found the solution!"
Agent THINKS: "I have enough information now"
Agent RESPONDS: "Follow KB-001 for password reset..."

âœ… Only used 2 tools instead of 4! (50% cost savings)
```

### 8.2 Build the ReAct Agent

Run this cell (this is the most important part!):

```python
from langgraph.prebuilt import create_react_agent
from langchain_core.messages import SystemMessage

# CRITICAL: Bind tools to the LLM
tools = [classify_ticket, extract_metadata, search_knowledge, query_historical]

llm_with_tools = llm.bind_tools(tools)  # â† THIS IS CRITICAL!

# Create system message
system_message = SystemMessage(content="""
You are an intelligent support ticket assistant.

Your job is to help analyze and respond to support tickets.

Available tools:
- classify_ticket: Use FIRST to understand priority and category
- extract_metadata: Use to pull out user emails, error codes, system names
- search_knowledge: Use to find solutions in the knowledge base
- query_historical: Use to find similar past tickets

Important guidelines:
1. ALWAYS classify the ticket first
2. Only use additional tools if needed for the specific question
3. For simple questions (greetings, general info), just respond directly
4. Be efficient - don't use unnecessary tools
5. Provide clear, actionable responses

Think step-by-step and explain your reasoning.
""")

# Create the ReAct agent
react_agent = create_react_agent(
    llm_with_tools,
    tools,
    messages_modifier=system_message
)

print("âœ… LangGraph ReAct Agent created!")
```

**ğŸ“ Understanding `bind_tools()`**:

```python
# âŒ WITHOUT bind_tools():
# - Tools not properly formatted for LLM
# - May return XML instead of JSON
# - Parsing errors and failures

llm_with_tools = llm.bind_tools(tools)
# âœ… WITH bind_tools():
# - Tools explicitly bound to LLM
# - Consistent JSON formatting
# - Reliable function calling
```

**This is the #1 cause of agent failures!** Always use `bind_tools()`.

### 8.3 Test the ReAct Agent

Run these test cases:

**Test 1: Simple Question (Should use NO tools)**
```python
result = react_agent.invoke({
    "messages": [{"role": "user", "content": "Hello, what can you help me with?"}]
})

print(result['messages'][-1].content)
```

**Expected**: Direct response, no tools used.

**Test 2: Classification Question (Should use 1-2 tools)**
```python
result = react_agent.invoke({
    "messages": [{"role": "user", "content": "Is this urgent: Database connection timeout"}]
})

# Print the agent's thought process
for msg in result['messages']:
    print(f"\n{msg.type.upper()}: {msg.content[:200]}")
```

**Expected output:**
```
USER: Is this urgent: Database connection timeout

AI: I'll classify this ticket to determine urgency

TOOL: classify_ticket

TOOL_RESULT: {"priority": "P2", "category": "Database"}

AI: Yes, this is urgent. It's a P2 (High priority) Database issue...
```

**Test 3: Complex Question (Should use 3-4 tools)**
```python
result = react_agent.invoke({
    "messages": [{"role": "user", "content": """
    Critical issue: Production order processing system is completely down.
    Error message: ERR-500 Internal Server Error.
    This is affecting all customers trying to place orders.
    What should we do?
    """}]
})

# Count tools used
tool_calls = [msg for msg in result['messages'] if msg.type == 'tool']
print(f"\nâœ… Agent used {len(tool_calls)} tools")
print(f"Response: {result['messages'][-1].content}")
```

**Expected**: Uses 3-4 tools (classify, extract, search KB, check history).

### 8.4 Understanding the Difference

Let's compare the two agents:

| Question | Sequential Agent | ReAct Agent | Savings |
|----------|-----------------|-------------|---------|
| "Hello" | 4 tools (unnecessary) | 0 tools | 100% |
| "Is this urgent?" | 4 tools | 1-2 tools | 50-75% |
| "Critical outage help" | 4 tools | 3-4 tools | 0-25% |

**Key insight**: ReAct agents are **intelligent** - they adapt to task complexity!

---

## ğŸ”¬ Step 9: Compare and Test

### 9.1 Side-by-Side Comparison

Run this comparison cell:

```python
import time

test_cases = [
    "Hello, how are you?",
    "Password reset for john@company.com",
    "CRITICAL: Production database is down! All users affected. Error: Connection timeout."
]

for i, test in enumerate(test_cases, 1):
    print(f"\n{'='*60}")
    print(f"TEST CASE {i}: {test}")
    print('='*60)
    
    # Sequential Agent
    print("\nğŸ”¸ SEQUENTIAL AGENT:")
    start = time.time()
    # (Would always use 4 tools)
    seq_time = time.time() - start
    print(f"â±ï¸ Time: {seq_time:.2f}s | Tools used: 4")
    
    # ReAct Agent
    print("\nğŸ”¹ REACT AGENT:")
    start = time.time()
    result = react_agent.invoke({"messages": [{"role": "user", "content": test}]})
    react_time = time.time() - start
    tools_used = len([m for m in result['messages'] if m.type == 'tool'])
    print(f"â±ï¸ Time: {react_time:.2f}s | Tools used: {tools_used}")
    
    # Calculate savings
    savings = ((4 - tools_used) / 4) * 100
    print(f"ğŸ’° Cost Savings: {savings:.0f}%")
```

### 9.2 Understanding the Results

You should see something like:

```
TEST CASE 1: Hello, how are you?
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¸ SEQUENTIAL AGENT:
â±ï¸ Time: 4.2s | Tools used: 4
ğŸ’° Cost: $0.0020

ğŸ”¹ REACT AGENT:
â±ï¸ Time: 0.8s | Tools used: 0
ğŸ’° Cost: $0.0003
ğŸ’° Cost Savings: 100%
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

TEST CASE 2: Password reset for john@company.com
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¸ SEQUENTIAL AGENT:
â±ï¸ Time: 5.1s | Tools used: 4
ğŸ’° Cost: $0.0020

ğŸ”¹ REACT AGENT:
â±ï¸ Time: 2.3s | Tools used: 2
ğŸ’° Cost: $0.0010
ğŸ’° Cost Savings: 50%
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

TEST CASE 3: CRITICAL: Production database down...
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¸ SEQUENTIAL AGENT:
â±ï¸ Time: 5.3s | Tools used: 4
ğŸ’° Cost: $0.0020

ğŸ”¹ REACT AGENT:
â±ï¸ Time: 4.8s | Tools used: 4
ğŸ’° Cost: $0.0019
ğŸ’° Cost Savings: 5%
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Overall: 40-60% average cost savings with ReAct!
```

**ğŸ“ Key Takeaways**:
1. Simple tasks: ReAct is MUCH faster and cheaper
2. Medium tasks: ReAct is moderately better
3. Complex tasks: Both agents similar (both need all tools)
4. **Average savings: 40-60%** in real-world usage!

---

## ğŸ› Troubleshooting

### Common Issues

#### 1. "Tool not found" Error

**Problem:**
```
Error: Tool 'classify_ticket' not found
```

**Solution:**
- Verify tool name matches exactly in the tool list
- Check you ran the cell that defines the tool
- Ensure tools list includes all tools:
  ```python
  tools = [classify_ticket, extract_metadata, search_knowledge, query_historical]
  ```

#### 2. XML Format Errors

**Problem:**
```
Error: Expected JSON, got XML format
```

**Solution:**
- Make sure you're using `llm.bind_tools(tools)`!
  ```python
  # âŒ BAD:
  agent = create_react_agent(llm, tools)
  
  # âœ… GOOD:
  llm_with_tools = llm.bind_tools(tools)
  agent = create_react_agent(llm_with_tools, tools)
  ```

#### 3. Authentication Errors

**Problem:**
```
403 Forbidden: Invalid credentials
```

**Solution:**
- Verify your cluster is running
- Check WorkspaceClient initialization:
  ```python
  from databricks.sdk import WorkspaceClient
  w = WorkspaceClient()  # Uses notebook context automatically
  ```

#### 4. Function Not Found

**Problem:**
```
AnalysisException: Function 'ai_classify' not found
```

**Solution:**
- Make sure you created the UC AI Functions (Step 3)
- Verify you're using the correct catalog/schema:
  ```python
  spark.sql("USE CATALOG langraph_tutorial")
  spark.sql("USE SCHEMA agents")
  ```

#### 5. Agent Loops Forever

**Problem:**
Agent keeps calling tools repeatedly without stopping.

**Solution:**
- Check your system message includes stopping criteria
- Set max_iterations:
  ```python
  result = react_agent.invoke(
      {"messages": [...]},
      {"recursion_limit": 10}  # Stop after 10 steps
  )
  ```

#### 6. Slow Performance

**Problem:**
Agent takes too long to respond.

**Solution:**
- Reduce `max_tokens` in LLM config
- Use a faster model
- Optimize tool descriptions (shorter = faster)
- Add caching for repeated queries

---

## ğŸ¯ Next Steps

### Congratulations! ğŸ‰

You've built your first LangGraph ReAct agent on Databricks!

### What You Learned

âœ… How to set up Databricks Unity Catalog  
âœ… How to create AI Functions  
âœ… How to build LangChain Tools  
âœ… How to create a sequential agent  
âœ… How to create a LangGraph ReAct agent  
âœ… Why `bind_tools()` is critical  
âœ… How agents adapt to task complexity  
âœ… Real-world cost savings (40-60%)  

### Level Up Your Skills

#### ğŸ”° Beginner Next Steps
1. **Customize the tools**:
   - Add a new category to classification
   - Modify extraction to find different fields
   - Add your own knowledge base articles

2. **Experiment with test cases**:
   - Try different ticket scenarios
   - Compare simple vs complex tasks
   - Measure performance differences

3. **Read the documentation**:
   - [`docs/LANGRAPH_FOR_FRIENDS.md`](docs/LANGRAPH_FOR_FRIENDS.md) - Simplified explanations
   - [`docs/QUICK_START_AGENT_NOTEBOOK.md`](docs/QUICK_START_AGENT_NOTEBOOK.md) - Quick reference

#### ğŸ“ Intermediate Next Steps
1. **Add more tools**:
   - Email notification tool
   - Slack message tool
   - Ticket assignment tool

2. **Improve the agent**:
   - Add memory (remember previous conversations)
   - Add human-in-the-loop (ask for confirmation)
   - Add error recovery (retry failed tools)

3. **Deep dive documentation**:
   - [`docs/LANGRAPH_ARCHITECTURE.md`](docs/LANGRAPH_ARCHITECTURE.md) - Full architecture
   - [`docs/LANGRAPH_BIND_TOOLS_EXPLAINED.md`](docs/LANGRAPH_BIND_TOOLS_EXPLAINED.md) - Technical deep dive

#### ğŸš€ Advanced Next Steps
1. **Build a production system**:
   - Switch to the full project: `git checkout main`
   - Explore the Streamlit dashboard
   - Study the production deployment

2. **Create your own agent**:
   - Customer service chatbot
   - Email classification system
   - Document processing pipeline

3. **Contribute back**:
   - Share your improvements
   - Write a blog post
   - Help other learners

### Resources

#### Documentation
- **LangChain**: [python.langchain.com](https://python.langchain.com)
- **LangGraph**: [langchain-ai.github.io/langgraph](https://langchain-ai.github.io/langgraph/)
- **Databricks**: [docs.databricks.com](https://docs.databricks.com)

#### Community
- **GitHub Issues**: [Report bugs or ask questions](https://github.com/bigdatavik/databricks-ai-ticket-vectorsearch/issues)
- **Databricks Community**: [community.databricks.com](https://community.databricks.com)
- **LangChain Discord**: [discord.gg/langchain](https://discord.gg/langchain)

#### Full Project
- **Main Repository**: [databricks-ai-ticket-vectorsearch](https://github.com/bigdatavik/databricks-ai-ticket-vectorsearch)
- **Production Branch**: `main` - Full dashboard implementation
- **Tutorial Branch**: `langgraph-databricks-tutorial` - This tutorial

---

## ğŸ“ Summary

### What We Built

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LangGraph ReAct Agent on Databricks       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  Tools:                                     â”‚
â”‚  â”œâ”€ classify_ticket() â†’ Priority/Category  â”‚
â”‚  â”œâ”€ extract_metadata() â†’ Key information   â”‚
â”‚  â”œâ”€ search_knowledge() â†’ Find solutions    â”‚
â”‚  â””â”€ query_historical() â†’ Past tickets      â”‚
â”‚                                             â”‚
â”‚  Agent:                                     â”‚
â”‚  â”œâ”€ ReAct Pattern (Reasoning + Acting)     â”‚
â”‚  â”œâ”€ Adaptive tool selection                â”‚
â”‚  â”œâ”€ 40-60% cost savings                    â”‚
â”‚  â””â”€ Production-ready patterns              â”‚
â”‚                                             â”‚
â”‚  Platform:                                  â”‚
â”‚  â”œâ”€ Databricks Unity Catalog               â”‚
â”‚  â”œâ”€ AI Functions (classify, extract, gen)  â”‚
â”‚  â”œâ”€ LangChain + LangGraph                  â”‚
â”‚  â””â”€ Deployed via Asset Bundles             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Patterns to Remember

1. **Always use `bind_tools()`**
   ```python
   llm_with_tools = llm.bind_tools(tools)
   ```

2. **Write clear tool descriptions**
   - The AI reads these to choose tools
   - Include: what it does, when to use it, what it returns

3. **Use Pydantic schemas**
   - Type safety
   - Better error messages
   - Self-documenting code

4. **Test tools individually first**
   - Verify each tool works
   - Then combine into agent

5. **Compare approaches**
   - Sequential for predictability
   - ReAct for efficiency
   - Choose based on your needs

---

## ğŸ™ Feedback & Support

### Found This Helpful?

â­ **Star the repository**: [databricks-ai-ticket-vectorsearch](https://github.com/bigdatavik/databricks-ai-ticket-vectorsearch)

ğŸ“ **Share your experience**:
- What worked well?
- What was confusing?
- What should we add?

### Need Help?

ğŸ’¬ **Ask questions**:
- [GitHub Issues](https://github.com/bigdatavik/databricks-ai-ticket-vectorsearch/issues)
- [GitHub Discussions](https://github.com/bigdatavik/databricks-ai-ticket-vectorsearch/discussions)

ğŸ› **Report bugs**:
- [Submit an issue](https://github.com/bigdatavik/databricks-ai-ticket-vectorsearch/issues/new)

ğŸ¤ **Contribute**:
- Improve this tutorial
- Add more examples
- Fix typos or errors

---

**Built with â¤ï¸ for the Databricks Community**

**Happy Learning! ğŸš€**

---

*Last Updated: November 2024*  
*Tutorial Version: 1.0*  
*Databricks Runtime: 16.4 LTS or later*

